# -------------------------------------------------------
# SISTEMA DE RECOMENDACI√ìN OPTIMIZADO (TF-IDF + LSA)
# CON CACH√â INTELIGENTE Y DETECCI√ìN AUTOM√ÅTICA DE CAMBIOS
# Autor: Tadeo Manuel Portillo Guzm√°n (Optimizado)
# Proyecto: Plataforma "Destinos Turismo"
# -------------------------------------------------------

import os
import pandas as pd
import joblib
import hashlib
import time
from functools import lru_cache
from sqlalchemy import create_engine
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords
from pathlib import Path
from dotenv import load_dotenv

# ------------------------------------------
# 1Ô∏è‚É£ CONFIGURACI√ìN DE CONEXI√ìN A LA BD
# ------------------------------------------
env_path = Path(__file__).resolve().parent / ".env"
load_dotenv(dotenv_path=env_path)


DB_USER = os.environ.get("DB_USER")
DB_PASS = os.environ.get("DB_PASS")
DB_HOST = os.environ.get("DB_HOST", "localhost")
DB_NAME = os.environ.get("DB_NAME", "impplacc_destinos")

engine = create_engine(f"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME}")

# Carpeta donde se guardar√°n los modelos
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

# Rutas de los archivos de modelo (OPTIMIZADO: agregamos similarity y hash)
VECTORIZER_PATH = os.path.join(MODEL_DIR, "vectorizer.joblib")
LSA_PATH = os.path.join(MODEL_DIR, "lsa_model.joblib")
DF_PATH = os.path.join(MODEL_DIR, "revistas_df.joblib")
SIMILARITY_PATH = os.path.join(MODEL_DIR, "similarity_matrix.joblib")  # ‚≠ê NUEVO
ID_INDEX_PATH = os.path.join(MODEL_DIR, "id_index.joblib")  # ‚≠ê NUEVO
DATA_HASH_PATH = os.path.join(MODEL_DIR, "data_hash.txt")  # ‚≠ê NUEVO


# ------------------------------------------
# 2Ô∏è‚É£ FUNCI√ìN PARA CALCULAR HASH DE LOS DATOS
# ------------------------------------------
def calcular_hash_datos(df):
    """
    Genera un hash MD5 de los IDs y t√≠tulos para detectar cambios.
    M√°s robusto que solo contar registros.
    """
    datos_unicos = df[["id", "title"]].to_string()
    return hashlib.md5(datos_unicos.encode()).hexdigest()


# ------------------------------------------
# 3Ô∏è‚É£ FUNCI√ìN DE ENTRENAMIENTO OPTIMIZADA
# ------------------------------------------
def entrenar_modelo():
    print("\nüß† Entrenando modelo TF-IDF + LSA optimizado...")
    start_time = time.time()

    query = """
    SELECT 
        id,
        title,
        COALESCE(description, '') AS description,
        COALESCE(keywords, '') AS keywords,
        COALESCE(topics, '') AS topics,
        COALESCE(region, '') AS region
    FROM magazine;
    """

    df = pd.read_sql(query, engine).fillna("")

    # Combinar campos sem√°nticos
    df["texto_final"] = (
        df["title"]
        + " "
        + df["description"]
        + " "
        + df["keywords"]
        + " "
        + df["topics"]
        + " "
        + df["region"]
    )

    # Descargar stopwords
    nltk.download("stopwords", quiet=True)
    spanish_stopwords = stopwords.words("spanish")

    # ‚≠ê OPTIMIZACI√ìN: Ajustar hiperpar√°metros din√°micamente
    n_registros = len(df)
    max_features = min(3000, n_registros * 10)

    print(f"üìä Dataset: {n_registros} revistas")

    # TF-IDF (sin min_df para datasets peque√±os)
    vectorizer = TfidfVectorizer(
        stop_words=spanish_stopwords,
        max_features=max_features,
        min_df=1,  # ‚≠ê Ajustado para datasets peque√±os
        max_df=0.95,  # ‚≠ê M√°s permisivo
    )
    tfidf_matrix = vectorizer.fit_transform(df["texto_final"])

    # ‚≠ê CR√çTICO: n_components debe ser menor que n_features
    n_features_real = tfidf_matrix.shape[1]
    n_components = min(100, max(10, n_features_real - 1))  # Siempre menor que features

    print(f"üîß Features extra√≠dos: {n_features_real}")
    print(f"üîß Componentes LSA: {n_components}")

    # LSA
    lsa = TruncatedSVD(n_components=n_components, random_state=42)
    lsa_matrix = lsa.fit_transform(tfidf_matrix)

    # ‚≠ê OPTIMIZACI√ìN: Calcular y guardar matriz de similitud
    print("üîÑ Calculando matriz de similitud...")
    similaridades = cosine_similarity(lsa_matrix)

    # ‚≠ê OPTIMIZACI√ìN: Crear √≠ndice de IDs para b√∫squeda O(1)
    id_to_idx = {id_val: idx for idx, id_val in enumerate(df["id"])}

    # Calcular hash de los datos
    data_hash = calcular_hash_datos(df)

    # Guardar todos los artefactos
    joblib.dump(vectorizer, VECTORIZER_PATH)
    joblib.dump(lsa, LSA_PATH)
    joblib.dump(df, DF_PATH)
    joblib.dump(similaridades, SIMILARITY_PATH)  # ‚≠ê NUEVO
    joblib.dump(id_to_idx, ID_INDEX_PATH)  # ‚≠ê NUEVO

    with open(DATA_HASH_PATH, "w") as f:
        f.write(data_hash)

    elapsed = time.time() - start_time
    print(f"‚úÖ Modelos entrenados y guardados en {elapsed:.2f}s\n")

    return df, vectorizer, lsa, similaridades, id_to_idx


# ------------------------------------------
# 4Ô∏è‚É£ DETECCI√ìN INTELIGENTE DE CAMBIOS
# ------------------------------------------
def necesita_reentrenamiento():
    """
    Retorna True si:
    - No existen los modelos
    - El hash de los datos cambi√≥ (nuevos/modificados/eliminados registros)
    """
    # Verificar existencia de archivos
    archivos_necesarios = [
        VECTORIZER_PATH,
        LSA_PATH,
        DF_PATH,
        SIMILARITY_PATH,
        ID_INDEX_PATH,
        DATA_HASH_PATH,
    ]

    if not all(os.path.exists(f) for f in archivos_necesarios):
        print("‚öôÔ∏è Modelos no encontrados. Se entrenar√° por primera vez.")
        return True

    # Cargar DataFrame guardado
    df_guardado = joblib.load(DF_PATH)

    # Cargar hash anterior
    with open(DATA_HASH_PATH, "r") as f:
        hash_anterior = f.read().strip()

    # Consultar datos actuales de la BD
    query = """
    SELECT 
        id,
        title,
        COALESCE(description, '') AS description,
        COALESCE(keywords, '') AS keywords,
        COALESCE(topics, '') AS topics,
        COALESCE(region, '') AS region
    FROM magazine;
    """
    df_actual = pd.read_sql(query, engine).fillna("")

    # Calcular hash actual
    hash_actual = calcular_hash_datos(df_actual[["id", "title"]])

    # Comparar hashes
    if hash_actual != hash_anterior:
        num_anterior = len(df_guardado)
        num_actual = len(df_actual)
        print(f"üìà Cambios detectados en la tabla magazine")
        print(f"   Registros: {num_anterior} ‚Üí {num_actual}")
        print(f"   Hash anterior: {hash_anterior[:8]}...")
        print(f"   Hash actual: {hash_actual[:8]}...")
        return True

    print("‚úÖ No se detectaron cambios en la tabla magazine.")
    return False


# ------------------------------------------
# 5Ô∏è‚É£ FUNCI√ìN DE RECOMENDACI√ìN OPTIMIZADA
# ------------------------------------------
@lru_cache(maxsize=200)  # ‚≠ê CACH√â para recomendaciones frecuentes
def recomendar_revistas(id_revista, top_k=5):
    """
    Retorna las revistas m√°s similares a una revista dada.
    OPTIMIZADO con b√∫squeda O(1) y cach√© LRU.
    """
    # ‚≠ê OPTIMIZACI√ìN: B√∫squeda O(1) con hash map
    if id_revista not in id_to_idx:
        print("‚ùå El ID de revista no existe en la base de datos.")
        return None

    idx = id_to_idx[id_revista]

    # Obtener similitudes (ya precalculadas)
    sim_scores = list(enumerate(similaridades[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Excluir la revista original
    sim_scores = [s for s in sim_scores if s[0] != idx]
    top_similares = sim_scores[:top_k]

    recomendaciones = df.iloc[[i for i, _ in top_similares]][
        ["id", "title", "region"]
    ].copy()
    recomendaciones["similaridad"] = [round(s, 3) for _, s in top_similares]

    return recomendaciones


# ------------------------------------------
# 6Ô∏è‚É£ CARGA O REENTRENAMIENTO AUTOM√ÅTICO
# ------------------------------------------
if necesita_reentrenamiento():
    df, vectorizer, lsa, similaridades, id_to_idx = entrenar_modelo()
    # Limpiar cach√© de recomendaciones
    recomendar_revistas.cache_clear()
else:
    print("üíæ Cargando modelos existentes...")
    start_time = time.time()

    vectorizer = joblib.load(VECTORIZER_PATH)
    lsa = joblib.load(LSA_PATH)
    df = joblib.load(DF_PATH)
    similaridades = joblib.load(SIMILARITY_PATH)  # ‚≠ê CARGA DIRECTA
    id_to_idx = joblib.load(ID_INDEX_PATH)  # ‚≠ê CARGA DIRECTA

    elapsed = time.time() - start_time
    print(f"‚úÖ Modelos cargados en {elapsed:.2f}s\n")


# ------------------------------------------
# 7Ô∏è‚É£ FUNCI√ìN DE ESTAD√çSTICAS DEL SISTEMA
# ------------------------------------------
def mostrar_estadisticas():
    """Muestra informaci√≥n sobre el sistema y el modelo cargado."""
    print("\n" + "=" * 60)
    print("üìä ESTAD√çSTICAS DEL SISTEMA DE RECOMENDACI√ìN")
    print("=" * 60)
    print(f"Total de revistas: {len(df)}")
    print(f"Dimensiones TF-IDF: {vectorizer.max_features}")
    print(f"Componentes LSA: {lsa.n_components}")
    print(f"Tama√±o matriz similitud: {similaridades.shape}")
    print(f"Cach√© de recomendaciones: {recomendar_revistas.cache_info()}")
    print("=" * 60 + "\n")


# ------------------------------------------
# 8Ô∏è‚É£ EJEMPLO DE USO
# ------------------------------------------
if __name__ == "__main__":
    mostrar_estadisticas()

    print("üîç SISTEMA DE RECOMENDACI√ìN DE REVISTAS (OPTIMIZADO)\n")

    try:
        ejemplo_id = int(input("Ingrese el ID de la revista para recomendar: "))

        start_time = time.time()
        resultado = recomendar_revistas(ejemplo_id, top_k=5)
        elapsed = time.time() - start_time

        if resultado is not None:
            print("\nüß≠ Revistas m√°s similares:\n")
            print(resultado.to_string(index=False))
            print(f"\n‚è±Ô∏è  Tiempo de respuesta: {elapsed*1000:.2f}ms")

            # Mostrar si vino del cach√©
            cache_info = recomendar_revistas.cache_info()
            if cache_info.hits > 0:
                print(f"üíæ (Servido desde cach√©)")

    except ValueError:
        print("\n‚ö†Ô∏è Por favor ingrese un n√∫mero v√°lido.")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Error: {e}")
